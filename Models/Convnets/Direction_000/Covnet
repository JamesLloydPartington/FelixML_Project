from keras import layers
from keras import models
from keras import optimizers
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing import image_dataset_from_directory
from keras.callbacks import EarlyStopping, ModelCheckpoint
#from keras.models import load_model
import os
import matplotlib.pyplot as plt

os.environ["TF_CPP_MIN_LOG_LEVEL"]="3" 

model = models.Sequential()
#model.add(layers.experimental.preprocessing.Rescaling(1./255))
model.add(layers.SeparableConv2D(8, (3, 3), activation='relu', input_shape=(128, 128, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.SeparableConv2D(32, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.SeparableConv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.SeparableConv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(512, activation='relu', kernel_regularizer='l2'))
model.add(layers.Dense(10, activation='softmax', kernel_regularizer='l2'))

model.compile(loss='categorical_crossentropy', optimizer="rmsprop", metrics=['acc'])

#model.summary()

base_dir = "F:\\training3000\\pngs_thick"

"""
train_ds = image_dataset_from_directory(base_dir,
                            image_size=(128, 128),
                            validation_split=0.2,
                            seed= 123,
                            subset="training",
                            batch_size=64,
                            label_mode="categorical")
val_ds = image_dataset_from_directory(base_dir,
                            image_size=(128, 128),
                            validation_split=0.2,
                            seed= 123,
                            subset="validation",
                            batch_size=64,
                            label_mode="categorical")
"""
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

train_dir = os.path.join(base_dir, "train")
val_dir = os.path.join(base_dir, "val")

train_generator = train_datagen.flow_from_directory(train_dir, target_size=(128, 128), batch_size=128, color_mode="grayscale")
val_generator = val_datagen.flow_from_directory(val_dir, target_size=(128, 128), batch_size=128, color_mode="grayscale")

EarlyStop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)
Checkpoint = ModelCheckpoint("Best_small_filters_png_thickness_model.hdf5", monitor='val_loss', verbose=1, save_best_only=True, mode='min', period=1)

#model = models.load_model('F:\\training3000\\pngs_thicknesses_model_L2.h5')
#model.compile(loss='categorical_crossentropy', optimizer="rmsprop", metrics=['acc'])

history = model.fit(train_generator, epochs=100, validation_data=val_generator, validation_steps=50, callbacks=[EarlyStop, Checkpoint])

model.save('F:/training3000/small_filters_pngs_thicknesses_model_L2.h5')

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()
