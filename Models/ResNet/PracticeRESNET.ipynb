{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image_dataset_from_directory\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils import Sequence\n",
    "from keras.models import load_model\n",
    "from tensorflow.distribute import MirroredStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All paths\n",
    "\n",
    "Path = \"/home/ug-ml/felix-ML/DataGenerator2/Data\" #Path where training and validation data is\n",
    "SaveDataPath = \"/home/ug-ml/Documents/GitHub_BigFiles/SaveFolder\" #Base directory of place you store information of models\n",
    "CifFlolder = \"/home/ug-ml/felix-ML/DataGenerator2/CifFolder\"\n",
    "SaveFolderName = \"/Convnet6\" #Will create a folder and put in information about the outcome / inputs\n",
    "ModelName = \"/Convnet6.hdf5\"\n",
    "\n",
    "\n",
    "#Many variables\n",
    "\n",
    "#Model Variables\n",
    "input_shape = (36, 128, 128)\n",
    "\n",
    "#Hyper parameters\n",
    "learning_rate = 0.0005\n",
    "l2_regularizer = 0.0001\n",
    "loss = 'categorical_crossentropy'\n",
    "optimizer = \"RMSprop\" #Not a variable ONLY used for a note\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "ShuffleTrainData = True\n",
    "\n",
    "#Call back variables\n",
    "TrainingPatience = 20\n",
    "CheckPointMonitor = 'val_acc'\n",
    "EarlyStopMonitor = 'val_acc'\n",
    "\n",
    "#CPU variables\n",
    "CPUworkers = 16\n",
    "\n",
    "#Limit range\n",
    "LatticeRange = [6, 11]\n",
    "\n",
    "\n",
    "#List the name of the variables you want to save in a file\n",
    "VariableListName = [\"input_shape\", \n",
    "                   \"learning_rate\", \"l2_regularizer\", \"loss\", \"optimizer\", \"batch_size\", \"epochs\", \"ShuffleTrainData\",\n",
    "                   \"TrainingPatience\", \"CheckPointMonitor\", \"EarlyStopMonitor\",\n",
    "                   \"CPUworkers\",\n",
    "                   \"LatticeRange\"]\n",
    "\n",
    "#List the variables in the same order as VariableListName\n",
    "VariableListValues = [input_shape, \n",
    "                   learning_rate, l2_regularizer, loss, optimizer, batch_size, epochs, ShuffleTrainData,\n",
    "                   TrainingPatience, CheckPointMonitor, EarlyStopMonitor,\n",
    "                    CPUworkers,\n",
    "                      LatticeRange]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder failed to be created, it may already exist\n"
     ]
    }
   ],
   "source": [
    "#Early stopping and check points\n",
    "\n",
    "EarlyStop = EarlyStopping(monitor = EarlyStopMonitor,\n",
    "                          mode = 'auto',\n",
    "                          verbose = 1,\n",
    "                          patience = TrainingPatience)\n",
    "\n",
    "NewPath = SaveDataPath + SaveFolderName\n",
    "Checkpoint = ModelCheckpoint(NewPath + ModelName, #Save path\n",
    "                             monitor = CheckPointMonitor,\n",
    "                             verbose = 1,\n",
    "                             save_best_only = True,\n",
    "                             mode = 'auto',\n",
    "                             save_freq = 'epoch')\n",
    "\n",
    "\n",
    "#Make folder to put model and history information\n",
    "try:\n",
    "    os.mkdir(NewPath)\n",
    "except:\n",
    "    print(\"Folder failed to be created, it may already exist\")\n",
    "    \n",
    "File1  = open(NewPath +\"/Parameters.txt\", \"w+\")\n",
    "if(len(VariableListName) == len(VariableListValues)):\n",
    "    for i in range(0, len(VariableListName)):\n",
    "        File1.write(VariableListName[i] + \" \" + str(VariableListValues[i]) + \"\\n\")\n",
    "    File1.close()\n",
    "else:\n",
    "    print(\"VariableListName and VariableListValues do not match up, so file can not be saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNN\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img1 (InputLayer)               [(None, 36, 128, 128 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_18 (SeparableC (None, 128, 128, 128 5060        img1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 128, 64, 64)  0           separable_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_19 (SeparableC (None, 128, 64, 64)  17664       max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 128, 64, 64)  0           separable_conv2d_19[0][0]        \n",
      "                                                                 max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_20 (SeparableC (None, 128, 62, 62)  17664       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 128, 31, 31)  0           separable_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 123008)       0           max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 123008)       0           flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 64)           7872576     dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 10)           650         dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,913,614\n",
      "Trainable params: 7,913,614\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Build model\n",
    "#strategy = MirroredStrategy() #Allows multiple GPUs\n",
    "\n",
    "#with strategy.scope():\n",
    "inputs = keras.Input(shape = (36, 128, 128), name = \"img1\")\n",
    "x = layers.SeparableConv2D(128, (3, 3), activation='relu', data_format='channels_first', padding = \"same\")(inputs)\n",
    "#x = layers.SeparableConv2D(128, (3, 3), activation='relu', data_format='channels_first', padding = \"same\")(x)\n",
    "block_1_output = layers.MaxPooling2D((2, 2), data_format='channels_first')(x)\n",
    "\n",
    "x = layers.SeparableConv2D(128, (3, 3), activation='relu', data_format='channels_first', padding = \"same\")(block_1_output)\n",
    "#x = layers.SeparableConv2D(128, (3, 3), activation='relu', data_format='channels_first', padding = \"same\")(x)\n",
    "block_2_output = layers.add([x, block_1_output])\n",
    "\n",
    "#x = layers.Conv2D(128, (3, 3), activation='relu', data_format='channels_first', padding = \"same\")(block_2_output)\n",
    "#x = layers.Conv2D(128, (3, 3), activation='relu', data_format='channels_first', padding = \"same\")(x)\n",
    "#block_3_output = layers.add([x, block_2_output])\n",
    "\n",
    "x = layers.SeparableConv2D(128, (3, 3), activation='relu', data_format='channels_first')(block_2_output)\n",
    "#x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.MaxPooling2D((2, 2), data_format='channels_first')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(10)(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = keras.Model(inputs, outputs, name = \"ResNN\")\n",
    "with open(NewPath + '/summary.txt','w') as fh:\n",
    "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data done.\n",
      "training_seq done.\n",
      "val_images done.\n",
      "val_lab done.\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "#Load data generators\n",
    "\n",
    "class FelixSequence(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size, file_type):\n",
    "        \"\"\"Here self.x is a list of paths to file_type files. self.y is a\n",
    "        corresponding list of labels.\"\"\"\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.file_type = file_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return arrs_from_paths(batch_x, self.file_type), to_categorical(np.array(batch_y),10)\n",
    "\n",
    "def arrs_from_paths(paths, file_type):\n",
    "    if file_type == \"txt\":\n",
    "        return np.array([np.loadtxt(file_name) for file_name in paths])\n",
    "    elif file_type == \"npy\":\n",
    "        return np.array([np.load(file_name) for file_name in paths]) \n",
    "    \n",
    "    \n",
    "#Define Data gemerator\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "PathOfFile = CifFlolder +\"/FilePaths.txt\"\n",
    "with open(PathOfFile) as textFile:\n",
    "    lines = [line.split() for line in textFile]\n",
    "\n",
    "\n",
    "training_path = []\n",
    "training_labels = []\n",
    "validation_path = []\n",
    "validation_labels = []\n",
    "for i in lines:\n",
    "    PathSplit = i[0].split(\"/\")\n",
    "    for j in PathSplit:\n",
    "        if(j == \"training\"):\n",
    "            if(float(i[3]) > LatticeRange[0] and float(i[3]) < LatticeRange[1]):\n",
    "                training_path.append(i[0])\n",
    "                training_labels.append(int(i[2]))\n",
    "                break\n",
    "            \n",
    "        elif(j == \"validation\"):\n",
    "            if(float(i[3]) > LatticeRange[0] and float(i[3]) < LatticeRange[1]):\n",
    "                validation_path.append(i[0])\n",
    "                validation_labels.append(int(i[2]))\n",
    "                break\n",
    "\n",
    "                \n",
    "trainsize = len(training_path)\n",
    "validationsize = len(validation_path)\n",
    "\n",
    "ShuffleTraining = np.arange(trainsize, dtype = np.int)\n",
    "ShuffleValidation = np.arange(validationsize, dtype = np.int)\n",
    "\n",
    "training_path_shuffled = training_path.copy()\n",
    "training_labels_shuffled = training_labels.copy()\n",
    "validation_path_shuffled = validation_path.copy()\n",
    "validation_labels_shuffled = validation_labels.copy()\n",
    "\n",
    "rng.shuffle(ShuffleTraining)\n",
    "rng.shuffle(ShuffleValidation)\n",
    "\n",
    "for i in range(0, trainsize):\n",
    "    training_path_shuffled[i] = training_path[ShuffleTraining[i]]\n",
    "    training_labels_shuffled[i] = training_labels[ShuffleTraining[i]]\n",
    "    #print(training_path_shuffled[i], training_labels_shuffled[i])\n",
    "\n",
    "for i in range(0, validationsize):\n",
    "    validation_path_shuffled[i] = validation_path[ShuffleValidation[i]]\n",
    "    validation_labels_shuffled[i] = validation_labels[ShuffleValidation[i]]\n",
    "    #print(validation_path_shuffled[i], validation_labels_shuffled[i])\n",
    "    \n",
    "print(\"data done.\")\n",
    "training_seq = FelixSequence(training_path_shuffled, training_labels_shuffled, batch_size, \"npy\")\n",
    "print(\"training_seq done.\")\n",
    "val_images = arrs_from_paths(validation_path_shuffled, \"npy\")\n",
    "print(\"val_images done.\")\n",
    "val_lab = to_categorical(validation_labels_shuffled)\n",
    "print(\"val_lab done.\")\n",
    "print(\"Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0529 - acc: 0.0989\n",
      "Epoch 00001: val_acc improved from -inf to 0.10000, saving model to /home/ug-ml/Documents/GitHub_BigFiles/SaveFolder/Convnet6/Convnet6.hdf5\n",
      "370/370 [==============================] - 103s 279ms/step - loss: 8.0529 - acc: 0.0989 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 2/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00002: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 103s 279ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 3/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00003: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 102s 276ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 4/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00004: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 105s 283ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 5/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00005: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 103s 278ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 6/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00006: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 105s 283ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 7/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00007: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 102s 275ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 8/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00008: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 100s 270ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 9/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00009: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 104s 281ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 10/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00010: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 104s 281ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 11/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00011: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 102s 277ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 12/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00012: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 100s 271ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 13/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00013: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 103s 279ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 14/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00014: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 105s 284ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 15/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00015: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 99s 268ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 16/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00016: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 101s 272ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 17/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00017: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 103s 279ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 18/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00018: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 103s 278ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 19/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00019: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 105s 284ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n",
      "Epoch 20/20\n",
      "370/370 [==============================] - ETA: 0s - loss: 8.0618 - acc: 0.1000\n",
      "Epoch 00020: val_acc did not improve from 0.10000\n",
      "370/370 [==============================] - 102s 277ms/step - loss: 8.0618 - acc: 0.1000 - val_loss: 8.0590 - val_acc: 0.1000\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = loss,\n",
    "                  optimizer = optimizers.RMSprop(learning_rate = learning_rate),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "\n",
    "history = model.fit(training_seq, \n",
    "                    epochs=epochs, \n",
    "                    validation_data = (val_images, val_lab), \n",
    "                    callbacks=[EarlyStop, Checkpoint], \n",
    "                    workers = CPUworkers,\n",
    "                    shuffle = ShuffleTrainData,\n",
    "                    use_multiprocessing = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
