{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, layers, backend, Model, losses, datasets, models, metrics, optimizers, initializers\n",
    "from keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FelixSequence(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        \"\"\"Here self.x is a list of paths to .npy input files. self.y is a\n",
    "        corresponding list of paths to .npy output files.\"\"\"\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        #print(np.array([np.load(file_name) for file_name in batch_x]).shape, np.array([np.load(file_name) for file_name in batch_y]).shape)\n",
    "        return np.array([np.reshape(np.load(file_name), (128, 128, 1)) for file_name in batch_x]), np.array([np.reshape(np.load(file_name), (128, 128, 1)) for file_name in batch_y])\n",
    "    \n",
    "\n",
    "def gen_paths_labels(base_path = \"D:\\\\Uni Work\\\\Masters Project\\\\electron_dists\\\\Data\\\\VAE_000_1\\\\Data\"):\n",
    "    \"\"\"A generator to yield (data-paths, corresponding labels) tuples for each\n",
    "    segment of data (typically training, validation, and testing).\"\"\"\n",
    "    for segment in sorted(os.listdir(base_path)):\n",
    "        segment_path = os.path.join(base_path, segment)\n",
    "        input_paths = []\n",
    "        output_paths = []\n",
    "        for crystal in os.listdir(segment_path):\n",
    "            crystal_path = os.path.join(segment_path, crystal)\n",
    "            files = sorted(os.listdir(crystal_path))\n",
    "            input_paths.append(os.path.join(crystal_path, files[0]))\n",
    "            output_paths.append(os.path.join(crystal_path, files[1]))\n",
    "        yield [input_paths, output_paths]\n",
    "\n",
    "def gen_paths_fromfile(Path):\n",
    "    input_paths = []\n",
    "    output_paths = []\n",
    "    with open(Path) as textFile:\n",
    "        lines = [line.split() for line in textFile]\n",
    "    #print(lines)\n",
    "    for i in lines:\n",
    "        output_paths.append(i[0])\n",
    "        input_paths.append(i[0].split(\"Output.npy\")[0] + \"Input.npy\")\n",
    "    return([input_paths, output_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 10\n",
    "\n",
    "\"\"\"\n",
    "## Create a sampling layer\n",
    "\"\"\"\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Build the encoder\n",
    "\"\"\"\n",
    "\n",
    "def build_encoder(Dense_params = 1500000, Num_Kernals=8, Size_Kernals=8, latent_dim = 10):\n",
    "\n",
    "    encoder_inputs = Input(shape=(128, 128, 1))\n",
    "    Layer_Encode_1_K8 = layers.Conv2D(Num_Kernals, kernel_size = (Size_Kernals, Size_Kernals), activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "    Layer_Encode_2_K8 = layers.Conv2D(Num_Kernals, kernel_size = (Size_Kernals, Size_Kernals), activation=\"relu\", strides=2, padding=\"same\")(Layer_Encode_1_K8)\n",
    "\n",
    "    x = layers.Flatten()(Layer_Encode_2_K8)\n",
    "\n",
    "\n",
    "    DenseParam_Encode = Dense_params\n",
    "    DenseNeurons_Encode = int(DenseParam_Encode / (x.shape[1]))\n",
    "\n",
    "    x = layers.Dense(DenseNeurons_Encode, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\", kernel_initializer='zeros', bias_initializer='zeros')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    encoder.summary()\n",
    "    return encoder, encoder.layers[2].output_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Build the decoder\n",
    "\"\"\"\n",
    "\n",
    "def build_decoder(encoder_output_conv_shape, Dense_params = 1500000, Num_Kernals=8, Size_Kernals=8, latent_dim = 10):\n",
    "    Dense_Size = encoder_output_conv_shape\n",
    "    print(Dense_Size)\n",
    "    Dense_Depth = int(Dense_params / (latent_dim * Dense_Size * Dense_Size))\n",
    "\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(Dense_Size * Dense_Size * Dense_Depth, activation=\"relu\")(latent_inputs)\n",
    "    x = layers.Reshape((Dense_Size, Dense_Size, Dense_Depth))(x)\n",
    "\n",
    "    Layer_Decode_1_K8 = layers.Conv2DTranspose(Num_Kernals, kernel_size = (Size_Kernals, Size_Kernals), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "\n",
    "    Layer_Decode_2_K8 = layers.Conv2DTranspose(Num_Kernals, kernel_size = (Size_Kernals, Size_Kernals), activation=\"relu\", strides=2, padding=\"same\")(Layer_Decode_1_K8)\n",
    "\n",
    "\n",
    "    decoder_outputs = layers.Conv2DTranspose(1, kernel_size = (2, 2), activation=\"relu\", padding= \"same\")(Layer_Decode_2_K8)\n",
    "    decoder = Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "    decoder.summary()\n",
    "    return decoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(x)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                losses.mean_squared_logarithmic_error(y, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            #print(z_mean, z_log_var, z)\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result()\n",
    "        }\n",
    "\n",
    "    def call(self, data):\n",
    "        return self.decoder(self.encoder(data)[2])\n",
    "\n",
    "#losses.MSE(y, reconstruction), axis=(1, 2)\n",
    "#losses.mean_squared_logarithmic_error(y, reconstruction), axis=(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_params = dict(Dense_params=[500000, 1000000, 1500000],\n",
    "              Num_Kernals=[4, 8, 16],\n",
    "              Size_Kernals=[4, 8, 16],\n",
    "              latent_dim=[5, 10, 15])\n",
    "\n",
    "Dense_params_list = [500000, 1000000, 1500000]\n",
    "Num_Kernals_list = [4, 8, 16]\n",
    "Size_Kernals_list = [4, 8, 16]\n",
    "latent_dim_list = [5, 10, 15]\n",
    "\n",
    "batch_size_list = [32, 64, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 64, 64, 8)    520         input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 32, 32, 8)    4104        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 8192)         0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 61)           499773      flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 10)           620         dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 10)           620         dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sampling_14 (Sampling)          (None, 10)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 505,637\n",
      "Trainable params: 505,637\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "32\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 49152)             540672    \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DT (None, 64, 64, 8)         24584     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_19 (Conv2DT (None, 128, 128, 8)       4104      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_20 (Conv2DT (None, 128, 128, 1)       33        \n",
      "=================================================================\n",
      "Total params: 569,393\n",
      "Trainable params: 569,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 8)    520         input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 32, 8)    4104        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 8192)         0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 122)          999546      flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 10)           1230        dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 10)           1230        dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sampling_15 (Sampling)          (None, 10)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,006,630\n",
      "Trainable params: 1,006,630\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "32\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 99328)             1092608   \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 32, 32, 97)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_21 (Conv2DT (None, 64, 64, 8)         49672     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_22 (Conv2DT (None, 128, 128, 8)       4104      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_23 (Conv2DT (None, 128, 128, 1)       33        \n",
      "=================================================================\n",
      "Total params: 1,146,417\n",
      "Trainable params: 1,146,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 64, 64, 8)    520         input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 32, 8)    4104        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 8192)         0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 183)          1499319     flatten_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 10)           1840        dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 10)           1840        dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sampling_16 (Sampling)          (None, 10)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,507,623\n",
      "Trainable params: 1,507,623\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "32\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 149504)            1644544   \n",
      "_________________________________________________________________\n",
      "reshape_8 (Reshape)          (None, 32, 32, 146)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_24 (Conv2DT (None, 64, 64, 8)         74760     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_25 (Conv2DT (None, 128, 128, 8)       4104      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_26 (Conv2DT (None, 128, 128, 1)       33        \n",
      "=================================================================\n",
      "Total params: 1,723,441\n",
      "Trainable params: 1,723,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 64, 64, 4)    68          input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 32, 32, 4)    260         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 4096)         0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 366)          1499502     flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 10)           3670        dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 10)           3670        dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sampling_17 (Sampling)          (None, 10)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,507,170\n",
      "Trainable params: 1,507,170\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(<tensorflow.python.keras.engine.functional.Functional object at 0x7f231c04df28>, 32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-3ac5617df702>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvaes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mbuild_vaes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-3ac5617df702>\u001b[0m in \u001b[0;36mbuild_vaes\u001b[0;34m(Dense_params, Num_Kernals, Size_Kernals, latent_dim)\u001b[0m\n\u001b[1;32m     26\u001b[0m                                     \u001b[0mNum_Kernals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                     \u001b[0mSize_Kernals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                     latent_dim = 10)\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mvaes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-95babba015eb>\u001b[0m in \u001b[0;36mbuild_decoder\u001b[0;34m(encoder_output_conv_shape, Dense_params, Num_Kernals, Size_Kernals, latent_dim)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mDense_Size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output_conv_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense_Size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mDense_Depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense_params\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mDense_Size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mDense_Size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlatent_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'tuple'"
     ]
    }
   ],
   "source": [
    "def build_vaes(Dense_params = [1500000], \n",
    "              Num_Kernals=[8], \n",
    "              Size_Kernals=[8], \n",
    "              latent_dim = [10]):\n",
    "    vaes = []\n",
    "    for i in Dense_params:\n",
    "        encoder, encoder_output_conv_shape = build_encoder(Dense_params = i, \n",
    "                                Num_Kernals=8, \n",
    "                                Size_Kernals=8, \n",
    "                                latent_dim = 10)\n",
    "        decoder = build_decoder(encoder_output_conv_shape,\n",
    "                                Dense_params = i, \n",
    "                                Num_Kernals=8, \n",
    "                                Size_Kernals=8, \n",
    "                                latent_dim = 10)\n",
    "        vaes.append(VAE(encoder, decoder))\n",
    "        \n",
    "    for i in Num_Kernals:\n",
    "        for j in Size_Kernals:\n",
    "            encoder = build_encoder(Dense_params = 1500000, \n",
    "                                    Num_Kernals=i, \n",
    "                                    Size_Kernals=j, \n",
    "                                    latent_dim = 10)\n",
    "            print(i,j)\n",
    "            decoder = build_decoder(encoder,\n",
    "                                    Dense_params = 1500000, \n",
    "                                    Num_Kernals=i, \n",
    "                                    Size_Kernals=j, \n",
    "                                    latent_dim = 10)\n",
    "            vaes.append(VAE(encoder, decoder))\n",
    "\n",
    "    for i in latent_dim:\n",
    "        encoder = build_encoder(Dense_params = 1500000, \n",
    "                                Num_Kernals=8, \n",
    "                                Size_Kernals=8, \n",
    "                                latent_dim = i)\n",
    "        decoder = build_decoder(encoder,\n",
    "                                Dense_params = 1500000, \n",
    "                                Num_Kernals=8, \n",
    "                                Size_Kernals=8, \n",
    "                                latent_dim = i)\n",
    "        vaes.append(VAE(encoder, decoder))\n",
    "        \n",
    "    return vaes\n",
    "\n",
    "build_vaes(**dict_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "Epoch 0 / 1500 : \n",
      "Training: \n",
      "91/91 [==============================] - 10s 109ms/step - loss: 1165.7454 - reconstruction_loss: 1031.3103 - kl_loss: 4.6505\n",
      "Validation: \n",
      "The model improved from:  inf to:  443.7972717285156\n",
      "Average reconstruction loss:  443.7972717285156\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1500 : \n",
      "Training: \n",
      "Epoch 2/2\n",
      "91/91 [==============================] - 11s 120ms/step - loss: 394.6472 - reconstruction_loss: 353.6799 - kl_loss: 11.0013\n",
      "Validation: \n",
      "The model improved from:  443.7972717285156 to:  296.634765625\n",
      "Average reconstruction loss:  296.634765625\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 2 / 1500 : \n",
      "Training: \n",
      "Epoch 3/3\n",
      "91/91 [==============================] - 11s 123ms/step - loss: 310.3993 - reconstruction_loss: 283.3571 - kl_loss: 14.2534\n",
      "Validation: \n",
      "The model improved from:  296.634765625 to:  268.3129577636719\n",
      "Average reconstruction loss:  268.3129577636719\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 3 / 1500 : \n",
      "Training: \n",
      "Epoch 4/4\n",
      "91/91 [==============================] - 9s 95ms/step - loss: 268.4695 - reconstruction_loss: 246.6978 - kl_loss: 15.9995\n",
      "Validation: \n",
      "The model improved from:  268.3129577636719 to:  234.40365600585938\n",
      "Average reconstruction loss:  234.40365600585938\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 4 / 1500 : \n",
      "Training: \n",
      "Epoch 5/5\n",
      "91/91 [==============================] - 9s 99ms/step - loss: 238.5815 - reconstruction_loss: 216.1651 - kl_loss: 17.6500\n",
      "Validation: \n",
      "The model improved from:  234.40365600585938 to:  216.12603759765625\n",
      "Average reconstruction loss:  216.12603759765625\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 5 / 1500 : \n",
      "Training: \n",
      "Epoch 6/6\n",
      "91/91 [==============================] - 9s 101ms/step - loss: 217.3215 - reconstruction_loss: 196.3261 - kl_loss: 18.5621\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  220.02838134765625\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 6 / 1500 : \n",
      "Training: \n",
      "Epoch 7/7\n",
      " 2/91 [..............................] - ETA: 7s - loss: 203.1557 - reconstruction_loss: 184.3449 - kl_loss: 19.8998WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0365s vs `on_train_batch_end` time: 0.1184s). Check your callbacks.\n",
      "91/91 [==============================] - 9s 104ms/step - loss: 200.9225 - reconstruction_loss: 180.5427 - kl_loss: 18.9620\n",
      "Validation: \n",
      "The model improved from:  216.12603759765625 to:  192.73695373535156\n",
      "Average reconstruction loss:  192.73695373535156\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 7 / 1500 : \n",
      "Training: \n",
      "Epoch 8/8\n",
      "91/91 [==============================] - 9s 100ms/step - loss: 188.7044 - reconstruction_loss: 170.3236 - kl_loss: 19.6107\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  195.11216735839844\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 8 / 1500 : \n",
      "Training: \n",
      "Epoch 9/9\n",
      "91/91 [==============================] - 9s 102ms/step - loss: 178.8844 - reconstruction_loss: 154.9101 - kl_loss: 20.1168\n",
      "Validation: \n",
      "The model improved from:  192.73695373535156 to:  176.82546997070312\n",
      "Average reconstruction loss:  176.82546997070312\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 9 / 1500 : \n",
      "Training: \n",
      "Epoch 10/10\n",
      "91/91 [==============================] - 9s 102ms/step - loss: 164.9042 - reconstruction_loss: 146.0650 - kl_loss: 20.2032\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  181.56504821777344\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 10 / 1500 : \n",
      "Training: \n",
      "Epoch 11/11\n",
      "91/91 [==============================] - 8s 93ms/step - loss: 161.8479 - reconstruction_loss: 141.8045 - kl_loss: 20.5355 1s - loss: 161.5944 - recons\n",
      "Validation: \n",
      "The model did not improve, patience_i =  2\n",
      "Average reconstruction loss:  177.15521240234375\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 11 / 1500 : \n",
      "Training: \n",
      "Epoch 12/12\n",
      "91/91 [==============================] - 9s 101ms/step - loss: 158.7252 - reconstruction_loss: 134.6039 - kl_loss: 20.7409\n",
      "Validation: \n",
      "The model improved from:  176.82546997070312 to:  168.91098022460938\n",
      "Average reconstruction loss:  168.91098022460938\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 12 / 1500 : \n",
      "Training: \n",
      "Epoch 13/13\n",
      "91/91 [==============================] - 9s 101ms/step - loss: 146.5860 - reconstruction_loss: 127.0756 - kl_loss: 21.1451\n",
      "Validation: \n",
      "The model improved from:  168.91098022460938 to:  162.6965789794922\n",
      "Average reconstruction loss:  162.6965789794922\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 13 / 1500 : \n",
      "Training: \n",
      "Epoch 14/14\n",
      "91/91 [==============================] - 10s 108ms/step - loss: 142.4716 - reconstruction_loss: 122.3879 - kl_loss: 21.1553\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  164.87635803222656\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 14 / 1500 : \n",
      "Training: \n",
      "Epoch 15/15\n",
      "91/91 [==============================] - 10s 108ms/step - loss: 138.1288 - reconstruction_loss: 115.2433 - kl_loss: 21.2057\n",
      "Validation: \n",
      "The model improved from:  162.6965789794922 to:  156.23580932617188\n",
      "Average reconstruction loss:  156.23580932617188\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 15 / 1500 : \n",
      "Training: \n",
      "Epoch 16/16\n",
      "91/91 [==============================] - 10s 109ms/step - loss: 130.2855 - reconstruction_loss: 112.0251 - kl_loss: 21.4643\n",
      "Validation: \n",
      "The model improved from:  156.23580932617188 to:  155.16876220703125\n",
      "Average reconstruction loss:  155.16876220703125\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 16 / 1500 : \n",
      "Training: \n",
      "Epoch 17/17\n",
      "91/91 [==============================] - 10s 109ms/step - loss: 129.2784 - reconstruction_loss: 107.3093 - kl_loss: 21.5740\n",
      "Validation: \n",
      "The model improved from:  155.16876220703125 to:  152.08738708496094\n",
      "Average reconstruction loss:  152.08738708496094\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 17 / 1500 : \n",
      "Training: \n",
      "Epoch 18/18\n",
      "91/91 [==============================] - 10s 107ms/step - loss: 123.3097 - reconstruction_loss: 103.6810 - kl_loss: 21.6302\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  153.0104217529297\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 18 / 1500 : \n",
      "Training: \n",
      "Epoch 19/19\n",
      "91/91 [==============================] - 10s 113ms/step - loss: 122.6471 - reconstruction_loss: 99.9154 - kl_loss: 21.8832\n",
      "Validation: \n",
      "The model improved from:  152.08738708496094 to:  146.17079162597656\n",
      "Average reconstruction loss:  146.17079162597656\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 19 / 1500 : \n",
      "Training: \n",
      "Epoch 20/20\n",
      "91/91 [==============================] - 11s 125ms/step - loss: 117.3692 - reconstruction_loss: 97.9326 - kl_loss: 21.7522\n",
      "Validation: \n",
      "The model improved from:  146.17079162597656 to:  144.19126892089844\n",
      "Average reconstruction loss:  144.19126892089844\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 20 / 1500 : \n",
      "Training: \n",
      "Epoch 21/21\n",
      "91/91 [==============================] - 10s 110ms/step - loss: 113.7753 - reconstruction_loss: 94.6350 - kl_loss: 21.8945\n",
      "Validation: \n",
      "The model improved from:  144.19126892089844 to:  143.83169555664062\n",
      "Average reconstruction loss:  143.83169555664062\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 21 / 1500 : \n",
      "Training: \n",
      "Epoch 22/22\n",
      "91/91 [==============================] - 11s 118ms/step - loss: 111.1865 - reconstruction_loss: 92.1535 - kl_loss: 21.7795\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  144.3245849609375\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 22 / 1500 : \n",
      "Training: \n",
      "Epoch 23/23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 10s 112ms/step - loss: 110.6052 - reconstruction_loss: 89.1002 - kl_loss: 21.9632\n",
      "Validation: \n",
      "The model improved from:  143.83169555664062 to:  142.55430603027344\n",
      "Average reconstruction loss:  142.55430603027344\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 23 / 1500 : \n",
      "Training: \n",
      "Epoch 24/24\n",
      "91/91 [==============================] - 10s 113ms/step - loss: 108.9079 - reconstruction_loss: 88.3187 - kl_loss: 22.0709\n",
      "Validation: \n",
      "The model improved from:  142.55430603027344 to:  141.71836853027344\n",
      "Average reconstruction loss:  141.71836853027344\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 24 / 1500 : \n",
      "Training: \n",
      "Epoch 25/25\n",
      "91/91 [==============================] - 10s 114ms/step - loss: 105.8883 - reconstruction_loss: 85.7496 - kl_loss: 22.1343\n",
      "Validation: \n",
      "The model improved from:  141.71836853027344 to:  138.10838317871094\n",
      "Average reconstruction loss:  138.10838317871094\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 25 / 1500 : \n",
      "Training: \n",
      "Epoch 26/26\n",
      "91/91 [==============================] - 10s 110ms/step - loss: 104.8875 - reconstruction_loss: 82.8247 - kl_loss: 22.0205\n",
      "Validation: \n",
      "The model improved from:  138.10838317871094 to:  135.01010131835938\n",
      "Average reconstruction loss:  135.01010131835938\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 26 / 1500 : \n",
      "Training: \n",
      "Epoch 27/27\n",
      "91/91 [==============================] - 11s 121ms/step - loss: 105.7613 - reconstruction_loss: 82.6508 - kl_loss: 22.0881\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  135.41046142578125\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 27 / 1500 : \n",
      "Training: \n",
      "Epoch 28/28\n",
      "91/91 [==============================] - 11s 126ms/step - loss: 99.5522 - reconstruction_loss: 80.2749 - kl_loss: 22.2471\n",
      "Validation: \n",
      "The model improved from:  135.01010131835938 to:  134.74131774902344\n",
      "Average reconstruction loss:  134.74131774902344\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 28 / 1500 : \n",
      "Training: \n",
      "Epoch 29/29\n",
      "91/91 [==============================] - 10s 113ms/step - loss: 97.3675 - reconstruction_loss: 76.5663 - kl_loss: 22.1888\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  135.3065948486328\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 29 / 1500 : \n",
      "Training: \n",
      "Epoch 30/30\n",
      "91/91 [==============================] - 10s 114ms/step - loss: 98.1655 - reconstruction_loss: 76.4697 - kl_loss: 22.3585\n",
      "Validation: \n",
      "The model did not improve, patience_i =  2\n",
      "Average reconstruction loss:  137.43569946289062\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 30 / 1500 : \n",
      "Training: \n",
      "Epoch 31/31\n",
      "91/91 [==============================] - 11s 117ms/step - loss: 97.5559 - reconstruction_loss: 75.1590 - kl_loss: 22.3454\n",
      "Validation: \n",
      "The model improved from:  134.74131774902344 to:  133.60546875\n",
      "Average reconstruction loss:  133.60546875\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 31 / 1500 : \n",
      "Training: \n",
      "Epoch 32/32\n",
      "91/91 [==============================] - 11s 117ms/step - loss: 93.5444 - reconstruction_loss: 72.5901 - kl_loss: 22.2738\n",
      "Validation: \n",
      "The model improved from:  133.60546875 to:  130.817138671875\n",
      "Average reconstruction loss:  130.817138671875\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 32 / 1500 : \n",
      "Training: \n",
      "Epoch 33/33\n",
      "91/91 [==============================] - 11s 122ms/step - loss: 92.3383 - reconstruction_loss: 71.3765 - kl_loss: 22.4311\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  135.9154815673828\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 33 / 1500 : \n",
      "Training: \n",
      "Epoch 34/34\n",
      "91/91 [==============================] - 9s 104ms/step - loss: 93.2418 - reconstruction_loss: 71.5407 - kl_loss: 22.4184\n",
      "Validation: \n",
      "The model did not improve, patience_i =  2\n",
      "Average reconstruction loss:  131.09637451171875\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 34 / 1500 : \n",
      "Training: \n",
      "Epoch 35/35\n",
      "91/91 [==============================] - 10s 105ms/step - loss: 90.9789 - reconstruction_loss: 69.7198 - kl_loss: 22.3220\n",
      "Validation: \n",
      "The model improved from:  130.817138671875 to:  126.33460235595703\n",
      "Average reconstruction loss:  126.33460235595703\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 35 / 1500 : \n",
      "Training: \n",
      "Epoch 36/36\n",
      "91/91 [==============================] - 10s 106ms/step - loss: 90.7682 - reconstruction_loss: 70.5974 - kl_loss: 22.3163\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  129.32379150390625\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 36 / 1500 : \n",
      "Training: \n",
      "Epoch 37/37\n",
      "91/91 [==============================] - 10s 112ms/step - loss: 89.1889 - reconstruction_loss: 68.3905 - kl_loss: 22.3800\n",
      "Validation: \n",
      "The model did not improve, patience_i =  2\n",
      "Average reconstruction loss:  129.61837768554688\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 37 / 1500 : \n",
      "Training: \n",
      "Epoch 38/38\n",
      "91/91 [==============================] - 10s 110ms/step - loss: 89.1592 - reconstruction_loss: 67.4474 - kl_loss: 22.5648\n",
      "Validation: \n",
      "The model did not improve, patience_i =  3\n",
      "Average reconstruction loss:  127.18109893798828\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 38 / 1500 : \n",
      "Training: \n",
      "Epoch 39/39\n",
      "91/91 [==============================] - 10s 111ms/step - loss: 87.4117 - reconstruction_loss: 64.9368 - kl_loss: 22.4332\n",
      "Validation: \n",
      "The model improved from:  126.33460235595703 to:  124.66598510742188\n",
      "Average reconstruction loss:  124.66598510742188\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 39 / 1500 : \n",
      "Training: \n",
      "Epoch 40/40\n",
      "91/91 [==============================] - 11s 119ms/step - loss: 83.9986 - reconstruction_loss: 63.9730 - kl_loss: 22.4270s - loss: 83.5264 - reconstruction_\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  125.28555297851562\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 40 / 1500 : \n",
      "Training: \n",
      "Epoch 41/41\n",
      "91/91 [==============================] - 10s 111ms/step - loss: 82.6628 - reconstruction_loss: 63.3552 - kl_loss: 22.5630\n",
      "Validation: \n",
      "The model improved from:  124.66598510742188 to:  123.80388641357422\n",
      "Average reconstruction loss:  123.80388641357422\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 41 / 1500 : \n",
      "Training: \n",
      "Epoch 42/42\n",
      "91/91 [==============================] - 10s 111ms/step - loss: 81.5751 - reconstruction_loss: 60.9317 - kl_loss: 22.3887\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  125.97528839111328\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 42 / 1500 : \n",
      "Training: \n",
      "Epoch 43/43\n",
      "91/91 [==============================] - 10s 105ms/step - loss: 81.6681 - reconstruction_loss: 60.5167 - kl_loss: 22.5539\n",
      "Validation: \n",
      "The model improved from:  123.80388641357422 to:  122.67495727539062\n",
      "Average reconstruction loss:  122.67495727539062\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 43 / 1500 : \n",
      "Training: \n",
      "Epoch 44/44\n",
      "91/91 [==============================] - 10s 107ms/step - loss: 80.7921 - reconstruction_loss: 60.2950 - kl_loss: 22.5542\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  122.96184539794922\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 44 / 1500 : \n",
      "Training: \n",
      "Epoch 45/45\n",
      "91/91 [==============================] - 9s 102ms/step - loss: 83.5921 - reconstruction_loss: 60.2284 - kl_loss: 22.6156\n",
      "Validation: \n",
      "The model improved from:  122.67495727539062 to:  122.60826873779297\n",
      "Average reconstruction loss:  122.60826873779297\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 45 / 1500 : \n",
      "Training: \n",
      "Epoch 46/46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 10s 111ms/step - loss: 78.5721 - reconstruction_loss: 59.0185 - kl_loss: 22.5483\n",
      "Validation: \n",
      "The model improved from:  122.60826873779297 to:  122.0838623046875\n",
      "Average reconstruction loss:  122.0838623046875\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 46 / 1500 : \n",
      "Training: \n",
      "Epoch 47/47\n",
      "91/91 [==============================] - 11s 121ms/step - loss: 81.7776 - reconstruction_loss: 59.9753 - kl_loss: 22.5527s -\n",
      "Validation: \n",
      "The model improved from:  122.0838623046875 to:  121.94721984863281\n",
      "Average reconstruction loss:  121.94721984863281\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 47 / 1500 : \n",
      "Training: \n",
      "Epoch 48/48\n",
      "91/91 [==============================] - 11s 123ms/step - loss: 80.6071 - reconstruction_loss: 58.4990 - kl_loss: 22.5763s - loss:\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  123.10099792480469\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 48 / 1500 : \n",
      "Training: \n",
      "Epoch 49/49\n",
      "91/91 [==============================] - 11s 116ms/step - loss: 77.4117 - reconstruction_loss: 56.4553 - kl_loss: 22.6276\n",
      "Validation: \n",
      "The model did not improve, patience_i =  2\n",
      "Average reconstruction loss:  126.82015991210938\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 49 / 1500 : \n",
      "Training: \n",
      "Epoch 50/50\n",
      "91/91 [==============================] - 10s 110ms/step - loss: 78.9012 - reconstruction_loss: 56.3347 - kl_loss: 22.4996\n",
      "Validation: \n",
      "The model did not improve, patience_i =  3\n",
      "Average reconstruction loss:  128.47389221191406\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 50 / 1500 : \n",
      "Training: \n",
      "Epoch 51/51\n",
      "91/91 [==============================] - 10s 107ms/step - loss: 79.3071 - reconstruction_loss: 56.2025 - kl_loss: 22.6784\n",
      "Validation: \n",
      "The model improved from:  121.94721984863281 to:  121.40780639648438\n",
      "Average reconstruction loss:  121.40780639648438\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 51 / 1500 : \n",
      "Training: \n",
      "Epoch 52/52\n",
      "91/91 [==============================] - 10s 107ms/step - loss: 77.6743 - reconstruction_loss: 57.3566 - kl_loss: 22.4943\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  123.78060913085938\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 52 / 1500 : \n",
      "Training: \n",
      "Epoch 53/53\n",
      "91/91 [==============================] - 10s 113ms/step - loss: 76.5557 - reconstruction_loss: 55.1778 - kl_loss: 22.5470s - loss:\n",
      "Validation: \n",
      "The model improved from:  121.40780639648438 to:  119.30596160888672\n",
      "Average reconstruction loss:  119.30596160888672\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 53 / 1500 : \n",
      "Training: \n",
      "Epoch 54/54\n",
      "91/91 [==============================] - 10s 114ms/step - loss: 74.8154 - reconstruction_loss: 53.0294 - kl_loss: 22.6089\n",
      "Validation: \n",
      "The model improved from:  119.30596160888672 to:  119.28617858886719\n",
      "Average reconstruction loss:  119.28617858886719\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 54 / 1500 : \n",
      "Training: \n",
      "Epoch 55/55\n",
      "91/91 [==============================] - 10s 111ms/step - loss: 74.6421 - reconstruction_loss: 52.1700 - kl_loss: 22.5799\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  122.26921081542969\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 55 / 1500 : \n",
      "Training: \n",
      "Epoch 56/56\n",
      "91/91 [==============================] - 10s 115ms/step - loss: 74.8232 - reconstruction_loss: 53.7160 - kl_loss: 22.5572\n",
      "Validation: \n",
      "The model improved from:  119.28617858886719 to:  118.831298828125\n",
      "Average reconstruction loss:  118.831298828125\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 56 / 1500 : \n",
      "Training: \n",
      "Epoch 57/57\n",
      "91/91 [==============================] - 10s 109ms/step - loss: 73.2846 - reconstruction_loss: 52.1384 - kl_loss: 22.6758\n",
      "Validation: \n",
      "The model improved from:  118.831298828125 to:  117.41773223876953\n",
      "Average reconstruction loss:  117.41773223876953\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 57 / 1500 : \n",
      "Training: \n",
      "Epoch 58/58\n",
      "91/91 [==============================] - 11s 120ms/step - loss: 73.3797 - reconstruction_loss: 51.6622 - kl_loss: 22.6367\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  120.09304809570312\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 58 / 1500 : \n",
      "Training: \n",
      "Epoch 59/59\n",
      "91/91 [==============================] - 11s 117ms/step - loss: 71.8672 - reconstruction_loss: 50.1225 - kl_loss: 22.5615s - loss: 71.7232 - reconstruction_loss\n",
      "Validation: \n",
      "The model improved from:  117.41773223876953 to:  116.28050994873047\n",
      "Average reconstruction loss:  116.28050994873047\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 59 / 1500 : \n",
      "Training: \n",
      "Epoch 60/60\n",
      "91/91 [==============================] - 10s 112ms/step - loss: 72.3651 - reconstruction_loss: 50.0428 - kl_loss: 22.7090\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  116.5181884765625\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 60 / 1500 : \n",
      "Training: \n",
      "Epoch 61/61\n",
      "91/91 [==============================] - 10s 108ms/step - loss: 70.2011 - reconstruction_loss: 48.8629 - kl_loss: 22.5558\n",
      "Validation: \n",
      "The model did not improve, patience_i =  2\n",
      "Average reconstruction loss:  116.78036499023438\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 61 / 1500 : \n",
      "Training: \n",
      "Epoch 62/62\n",
      "91/91 [==============================] - 11s 118ms/step - loss: 70.7198 - reconstruction_loss: 48.5762 - kl_loss: 22.4918\n",
      "Validation: \n",
      "The model did not improve, patience_i =  3\n",
      "Average reconstruction loss:  116.47046661376953\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 62 / 1500 : \n",
      "Training: \n",
      "Epoch 63/63\n",
      "91/91 [==============================] - 11s 116ms/step - loss: 69.3646 - reconstruction_loss: 48.8483 - kl_loss: 22.6912\n",
      "Validation: \n",
      "The model did not improve, patience_i =  4\n",
      "Average reconstruction loss:  116.8516616821289\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 63 / 1500 : \n",
      "Training: \n",
      "Epoch 64/64\n",
      "91/91 [==============================] - 11s 122ms/step - loss: 69.5458 - reconstruction_loss: 47.7077 - kl_loss: 22.5810s - loss: 69.3342\n",
      "Validation: \n",
      "The model did not improve, patience_i =  5\n",
      "Average reconstruction loss:  120.04857635498047\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 64 / 1500 : \n",
      "Training: \n",
      "Epoch 65/65\n",
      "91/91 [==============================] - 10s 114ms/step - loss: 70.2215 - reconstruction_loss: 47.1040 - kl_loss: 22.6502\n",
      "Validation: \n",
      "The model did not improve, patience_i =  6\n",
      "Average reconstruction loss:  116.74848937988281\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 65 / 1500 : \n",
      "Training: \n",
      "Epoch 66/66\n",
      "91/91 [==============================] - 11s 118ms/step - loss: 70.1069 - reconstruction_loss: 49.8917 - kl_loss: 22.6378\n",
      "Validation: \n",
      "The model improved from:  116.28050994873047 to:  115.65068817138672\n",
      "Average reconstruction loss:  115.65068817138672\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 66 / 1500 : \n",
      "Training: \n",
      "Epoch 67/67\n",
      "91/91 [==============================] - 11s 116ms/step - loss: 69.5400 - reconstruction_loss: 47.1988 - kl_loss: 22.6048\n",
      "Validation: \n",
      "The model improved from:  115.65068817138672 to:  115.05878448486328\n",
      "Average reconstruction loss:  115.05878448486328\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 67 / 1500 : \n",
      "Training: \n",
      "Epoch 68/68\n",
      "91/91 [==============================] - 11s 119ms/step - loss: 68.9759 - reconstruction_loss: 47.3630 - kl_loss: 22.7183\n",
      "Validation: \n",
      "The model improved from:  115.05878448486328 to:  113.79680633544922\n",
      "Average reconstruction loss:  113.79680633544922\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 68 / 1500 : \n",
      "Training: \n",
      "Epoch 69/69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 11s 123ms/step - loss: 69.1025 - reconstruction_loss: 47.1485 - kl_loss: 22.5931\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  115.58760070800781\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 69 / 1500 : \n",
      "Training: \n",
      "Epoch 70/70\n",
      "91/91 [==============================] - 11s 121ms/step - loss: 67.3998 - reconstruction_loss: 46.2337 - kl_loss: 22.5723\n",
      "Validation: \n",
      "The model did not improve, patience_i =  2\n",
      "Average reconstruction loss:  116.79607391357422\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 70 / 1500 : \n",
      "Training: \n",
      "Epoch 71/71\n",
      "91/91 [==============================] - 10s 115ms/step - loss: 67.0632 - reconstruction_loss: 45.4846 - kl_loss: 22.7131\n",
      "Validation: \n",
      "The model did not improve, patience_i =  3\n",
      "Average reconstruction loss:  113.89315795898438\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 71 / 1500 : \n",
      "Training: \n",
      "Epoch 72/72\n",
      "91/91 [==============================] - 11s 121ms/step - loss: 69.6353 - reconstruction_loss: 46.6146 - kl_loss: 22.6021\n",
      "Validation: \n",
      "The model did not improve, patience_i =  4\n",
      "Average reconstruction loss:  119.40715789794922\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 72 / 1500 : \n",
      "Training: \n",
      "Epoch 73/73\n",
      "91/91 [==============================] - 10s 113ms/step - loss: 66.9036 - reconstruction_loss: 44.4943 - kl_loss: 22.5905\n",
      "Validation: \n",
      "The model did not improve, patience_i =  5\n",
      "Average reconstruction loss:  115.1812973022461\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 73 / 1500 : \n",
      "Training: \n",
      "Epoch 74/74\n",
      "91/91 [==============================] - 11s 124ms/step - loss: 65.5594 - reconstruction_loss: 44.3505 - kl_loss: 22.5419\n",
      "Validation: \n",
      "The model did not improve, patience_i =  6\n",
      "Average reconstruction loss:  114.65784454345703\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 74 / 1500 : \n",
      "Training: \n",
      "Epoch 75/75\n",
      "91/91 [==============================] - 11s 119ms/step - loss: 64.7193 - reconstruction_loss: 43.3723 - kl_loss: 22.4897\n",
      "Validation: \n",
      "The model improved from:  113.79680633544922 to:  112.68594360351562\n",
      "Average reconstruction loss:  112.68594360351562\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 75 / 1500 : \n",
      "Training: \n",
      "Epoch 76/76\n",
      "91/91 [==============================] - 11s 125ms/step - loss: 66.0391 - reconstruction_loss: 43.1584 - kl_loss: 22.5848\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  113.43380737304688\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 76 / 1500 : \n",
      "Training: \n",
      "Epoch 77/77\n",
      "91/91 [==============================] - 12s 128ms/step - loss: 64.0815 - reconstruction_loss: 43.2839 - kl_loss: 22.6152s - loss: 63.4843 - r\n",
      "Validation: \n",
      "The model did not improve, patience_i =  2\n",
      "Average reconstruction loss:  115.87747192382812\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 77 / 1500 : \n",
      "Training: \n",
      "Epoch 78/78\n",
      "91/91 [==============================] - 10s 113ms/step - loss: 65.5942 - reconstruction_loss: 43.7799 - kl_loss: 22.6321\n",
      "Validation: \n",
      "The model did not improve, patience_i =  3\n",
      "Average reconstruction loss:  116.76821899414062\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 78 / 1500 : \n",
      "Training: \n",
      "Epoch 79/79\n",
      "91/91 [==============================] - 11s 119ms/step - loss: 65.2274 - reconstruction_loss: 43.7752 - kl_loss: 22.6500s -\n",
      "Validation: \n",
      "The model did not improve, patience_i =  4\n",
      "Average reconstruction loss:  113.71338653564453\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 79 / 1500 : \n",
      "Training: \n",
      "Epoch 80/80\n",
      "91/91 [==============================] - 11s 119ms/step - loss: 65.6961 - reconstruction_loss: 42.9720 - kl_loss: 22.7131\n",
      "Validation: \n",
      "The model improved from:  112.68594360351562 to:  111.48779296875\n",
      "Average reconstruction loss:  111.48779296875\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 80 / 1500 : \n",
      "Training: \n",
      "Epoch 81/81\n",
      "91/91 [==============================] - 10s 115ms/step - loss: 65.6286 - reconstruction_loss: 42.9223 - kl_loss: 22.6770\n",
      "Validation: \n",
      "The model improved from:  111.48779296875 to:  111.45441436767578\n",
      "Average reconstruction loss:  111.45441436767578\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 81 / 1500 : \n",
      "Training: \n",
      "Epoch 82/82\n",
      "91/91 [==============================] - 11s 119ms/step - loss: 64.0876 - reconstruction_loss: 41.3867 - kl_loss: 22.5449s - loss: 64.1789 - r\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  112.64396667480469\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 82 / 1500 : \n",
      "Training: \n",
      "Epoch 83/83\n",
      "91/91 [==============================] - 11s 120ms/step - loss: 61.6831 - reconstruction_loss: 40.6716 - kl_loss: 22.5799\n",
      "Validation: \n",
      "The model did not improve, patience_i =  2\n",
      "Average reconstruction loss:  114.37982940673828\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 83 / 1500 : \n",
      "Training: \n",
      "Epoch 84/84\n",
      "91/91 [==============================] - 11s 124ms/step - loss: 62.9166 - reconstruction_loss: 41.4844 - kl_loss: 22.5778\n",
      "Validation: \n",
      "The model improved from:  111.45441436767578 to:  111.43766784667969\n",
      "Average reconstruction loss:  111.43766784667969\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 84 / 1500 : \n",
      "Training: \n",
      "Epoch 85/85\n",
      "91/91 [==============================] - 11s 126ms/step - loss: 62.4645 - reconstruction_loss: 40.6912 - kl_loss: 22.5455\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  115.37482452392578\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 85 / 1500 : \n",
      "Training: \n",
      "Epoch 86/86\n",
      "91/91 [==============================] - 11s 124ms/step - loss: 63.4661 - reconstruction_loss: 41.1524 - kl_loss: 22.6204\n",
      "Validation: \n",
      "The model did not improve, patience_i =  2\n",
      "Average reconstruction loss:  112.01399230957031\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 86 / 1500 : \n",
      "Training: \n",
      "Epoch 87/87\n",
      "91/91 [==============================] - 11s 120ms/step - loss: 62.7681 - reconstruction_loss: 40.9507 - kl_loss: 22.6720\n",
      "Validation: \n",
      "The model did not improve, patience_i =  3\n",
      "Average reconstruction loss:  112.4067611694336\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 87 / 1500 : \n",
      "Training: \n",
      "Epoch 88/88\n",
      "91/91 [==============================] - 11s 124ms/step - loss: 62.1506 - reconstruction_loss: 40.9326 - kl_loss: 22.5962\n",
      "Validation: \n",
      "The model improved from:  111.43766784667969 to:  110.39592742919922\n",
      "Average reconstruction loss:  110.39592742919922\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 88 / 1500 : \n",
      "Training: \n",
      "Epoch 89/89\n",
      "91/91 [==============================] - 11s 123ms/step - loss: 61.2597 - reconstruction_loss: 39.8497 - kl_loss: 22.5789\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  111.41073608398438\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 89 / 1500 : \n",
      "Training: \n",
      "Epoch 90/90\n",
      "91/91 [==============================] - 11s 122ms/step - loss: 62.5430 - reconstruction_loss: 40.8106 - kl_loss: 22.4775\n",
      "Validation: \n",
      "The model did not improve, patience_i =  2\n",
      "Average reconstruction loss:  114.70575714111328\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 90 / 1500 : \n",
      "Training: \n",
      "Epoch 91/91\n",
      "91/91 [==============================] - 10s 114ms/step - loss: 62.2015 - reconstruction_loss: 40.3870 - kl_loss: 22.5723\n",
      "Validation: \n",
      "The model did not improve, patience_i =  3\n",
      "Average reconstruction loss:  111.67174530029297\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 91 / 1500 : \n",
      "Training: \n",
      "Epoch 92/92\n",
      "91/91 [==============================] - 11s 116ms/step - loss: 62.0980 - reconstruction_loss: 40.6045 - kl_loss: 22.6330\n",
      "Validation: \n",
      "The model did not improve, patience_i =  4\n",
      "Average reconstruction loss:  112.8584213256836\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 92 / 1500 : \n",
      "Training: \n",
      "Epoch 93/93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 11s 117ms/step - loss: 61.3007 - reconstruction_loss: 40.1792 - kl_loss: 22.5345\n",
      "Validation: \n",
      "The model did not improve, patience_i =  5\n",
      "Average reconstruction loss:  113.00871276855469\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 93 / 1500 : \n",
      "Training: \n",
      "Epoch 94/94\n",
      "91/91 [==============================] - 7s 80ms/step - loss: 61.5406 - reconstruction_loss: 39.5999 - kl_loss: 22.5843\n",
      "Validation: \n",
      "The model did not improve, patience_i =  6\n",
      "Average reconstruction loss:  117.65890502929688\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 94 / 1500 : \n",
      "Training: \n",
      "Epoch 95/95\n",
      "91/91 [==============================] - 9s 98ms/step - loss: 61.4486 - reconstruction_loss: 38.9649 - kl_loss: 22.5433 0s - loss: 61.4399 - reconstruction_loss: 39.0056 - kl_l\n",
      "Validation: \n",
      "The model did not improve, patience_i =  7\n",
      "Average reconstruction loss:  111.99108123779297\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 95 / 1500 : \n",
      "Training: \n",
      "Epoch 96/96\n",
      "91/91 [==============================] - 11s 116ms/step - loss: 59.8997 - reconstruction_loss: 38.2025 - kl_loss: 22.5034\n",
      "Validation: \n",
      "The model did not improve, patience_i =  8\n",
      "Average reconstruction loss:  110.93126678466797\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 96 / 1500 : \n",
      "Training: \n",
      "Epoch 97/97\n",
      "91/91 [==============================] - 10s 113ms/step - loss: 62.6177 - reconstruction_loss: 38.9779 - kl_loss: 22.6720\n",
      "Validation: \n",
      "The model did not improve, patience_i =  9\n",
      "Average reconstruction loss:  111.14519500732422\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 97 / 1500 : \n",
      "Training: \n",
      "Epoch 98/98\n",
      "91/91 [==============================] - 10s 109ms/step - loss: 59.4982 - reconstruction_loss: 38.8330 - kl_loss: 22.5739\n",
      "Validation: \n",
      "The model did not improve, patience_i =  10\n",
      "Average reconstruction loss:  113.76813507080078\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 98 / 1500 : \n",
      "Training: \n",
      "Epoch 99/99\n",
      "91/91 [==============================] - 10s 115ms/step - loss: 61.0206 - reconstruction_loss: 38.1198 - kl_loss: 22.6380\n",
      "Validation: \n",
      "The model did not improve, patience_i =  11\n",
      "Average reconstruction loss:  114.67377471923828\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 99 / 1500 : \n",
      "Training: \n",
      "Epoch 100/100\n",
      "91/91 [==============================] - 11s 116ms/step - loss: 59.8921 - reconstruction_loss: 37.9106 - kl_loss: 22.4262\n",
      "Validation: \n",
      "The model did not improve, patience_i =  12\n",
      "Average reconstruction loss:  111.64633178710938\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 100 / 1500 : \n",
      "Training: \n",
      "Epoch 101/101\n",
      "91/91 [==============================] - 10s 111ms/step - loss: 59.7882 - reconstruction_loss: 38.3989 - kl_loss: 22.6519\n",
      "Validation: \n",
      "The model did not improve, patience_i =  13\n",
      "Average reconstruction loss:  110.99571990966797\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 101 / 1500 : \n",
      "Training: \n",
      "Epoch 102/102\n",
      "91/91 [==============================] - 11s 116ms/step - loss: 59.3561 - reconstruction_loss: 37.9984 - kl_loss: 22.4071\n",
      "Validation: \n",
      "The model did not improve, patience_i =  14\n",
      "Average reconstruction loss:  112.55717468261719\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 102 / 1500 : \n",
      "Training: \n",
      "Epoch 103/103\n",
      "91/91 [==============================] - 11s 119ms/step - loss: 61.9023 - reconstruction_loss: 39.0698 - kl_loss: 22.6338\n",
      "Validation: \n",
      "The model did not improve, patience_i =  15\n",
      "Average reconstruction loss:  112.05803680419922\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 103 / 1500 : \n",
      "Training: \n",
      "Epoch 104/104\n",
      "91/91 [==============================] - 10s 110ms/step - loss: 58.6185 - reconstruction_loss: 37.4101 - kl_loss: 22.4986\n",
      "Validation: \n",
      "The model did not improve, patience_i =  16\n",
      "Average reconstruction loss:  111.41570281982422\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 104 / 1500 : \n",
      "Training: \n",
      "Epoch 105/105\n",
      "91/91 [==============================] - 11s 116ms/step - loss: 58.8345 - reconstruction_loss: 37.1827 - kl_loss: 22.4925s - l\n",
      "Validation: \n",
      "The model improved from:  110.39592742919922 to:  108.30406951904297\n",
      "Average reconstruction loss:  108.30406951904297\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 105 / 1500 : \n",
      "Training: \n",
      "Epoch 106/106\n",
      "91/91 [==============================] - 11s 122ms/step - loss: 58.5418 - reconstruction_loss: 36.0145 - kl_loss: 22.5429\n",
      "Validation: \n",
      "The model improved from:  108.30406951904297 to:  108.2175064086914\n",
      "Average reconstruction loss:  108.2175064086914\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 106 / 1500 : \n",
      "Training: \n",
      "Epoch 107/107\n",
      "91/91 [==============================] - 11s 121ms/step - loss: 56.1334 - reconstruction_loss: 35.7186 - kl_loss: 22.4871\n",
      "Validation: \n",
      "The model did not improve, patience_i =  1\n",
      "Average reconstruction loss:  111.28300476074219\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 107 / 1500 : \n",
      "Training: \n",
      "Epoch 108/108\n",
      "91/91 [==============================] - 11s 122ms/step - loss: 58.8323 - reconstruction_loss: 36.6400 - kl_loss: 22.4099\n",
      "Validation: \n",
      "The model did not improve, patience_i =  2\n",
      "Average reconstruction loss:  109.364013671875\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 108 / 1500 : \n",
      "Training: \n",
      "Epoch 109/109\n",
      "21/91 [=====>........................] - ETA: 5s - loss: 55.8498 - reconstruction_loss: 34.9005 - kl_loss: 22.3433"
     ]
    }
   ],
   "source": [
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=optimizers.Adam())\n",
    "\n",
    "\n",
    "batch_size=64\n",
    "#data_path = \"/home/ug-ml/felix-ML/VAE_000/Data/FilePaths/\"\n",
    "data_path = \"/home/ug-ml/felix-ML/VAE_000/Data/Data/\"\n",
    "\n",
    "data = [i for i in gen_paths_labels(data_path)]\n",
    "\n",
    "val_seq = FelixSequence(data[2][0], data[2][1], batch_size)\n",
    "train_seq = FelixSequence(data[1][0], data[1][1], batch_size)\n",
    "test_seq = FelixSequence(data[0][0], data[0][1], batch_size)\n",
    "\n",
    "epochs = 1500\n",
    "patience = 50\n",
    "\n",
    "best_model_name = \"VAE_1\"\n",
    "\n",
    "patience_i = 0\n",
    "best_val_loss = np.inf\n",
    "\n",
    "#training and validation histories, containing [0] the total loss, [1] the reconstruction loss, and [2] the kl loss.\n",
    "val_hist = np.zeros(shape=(1,epochs))\n",
    "train_hist = np.zeros(shape=(3,epochs))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    print(\"Epoch\", epoch, \"/\", epochs, \": \")\n",
    "    print(\"Training: \")\n",
    "    hist = vae.fit(x = train_seq, shuffle=True, epochs = epoch+1, workers = 16, initial_epoch=epoch)\n",
    "    train_hist[0][epoch] = hist.history[\"loss\"][0]\n",
    "    train_hist[1][epoch] = hist.history[\"reconstruction_loss\"][0]\n",
    "    train_hist[2][epoch] = hist.history[\"kl_loss\"][0]\n",
    "    print(\"Validation: \")\n",
    "\n",
    "    tot_batch_recon_loss = 0\n",
    "    count = 0\n",
    "    for x, y in val_seq:\n",
    "        #rint(x.shape, y.shape)\n",
    "        count += 1\n",
    "        reconstruction = vae(x)\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                losses.mean_squared_logarithmic_error(y, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "        tot_batch_recon_loss += reconstruction_loss\n",
    "        #print(batch_log_loss)\n",
    "\n",
    "    avg_recon_loss = float(tot_batch_recon_loss/count)\n",
    "    if(avg_recon_loss < best_val_loss):\n",
    "        print(\"The model improved from: \",best_val_loss, \"to: \", avg_recon_loss)\n",
    "        vae.save(\"/home/ug-ml/felix-ML/VAE_000/Data/Models/V3/\"+str(best_model_name))\n",
    "        best_val_loss = avg_recon_loss\n",
    "        patience_i = 0\n",
    "    else:\n",
    "        patience_i+=1\n",
    "        print(\"The model did not improve, patience_i = \", patience_i)\n",
    "\n",
    "    print(\"Average reconstruction loss: \", avg_recon_loss)\n",
    "    val_hist[0][epoch] = avg_recon_loss\n",
    "    if(patience_i > patience):\n",
    "        print(\"Early Stopping, the model did not improve from: \", best_val_loss)\n",
    "        break\n",
    "\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vae = models.load_model(\"/home/ug-ml/felix-ML/VAE_000/Data/Models/VAE_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "average_loss = 0\n",
    "#data[0][0], data[0][1]\n",
    "for i in range(0, len(data[0][0])):\n",
    "    x = np.load(data[0][0][i])\n",
    "    y = np.load(data[0][1][i])\n",
    "    #x = np.load(data[0][i])\n",
    "    #y = np.load(data[1][i])\n",
    "    a = np.reshape(vae(np.reshape(x, (1, 128, 128, 1))), (128, 128))\n",
    "    #print(TestPaths[0][i])\n",
    "    log_loss = 0\n",
    "    for j in range(0, a.shape[0]):\n",
    "        for k in range(0, a.shape[1]):\n",
    "            log_loss+=(math.log(1+a[j][k]) - math.log(1+y[j][k])) ** 2\n",
    "    \n",
    "    if log_loss > 500:\n",
    "        print(\"Log loss is: \", log_loss)\n",
    "        average_loss+=log_loss\n",
    "        w=10\n",
    "        h=10\n",
    "        fig=plt.figure(figsize=(8, 8))\n",
    "        columns = 3\n",
    "        rows = 1\n",
    "        fig.add_subplot(rows, columns, 1)\n",
    "        plt.imshow(x)\n",
    "        fig.add_subplot(rows, columns, 2)\n",
    "        plt.imshow(y)\n",
    "        fig.add_subplot(rows, columns, 3)\n",
    "        plt.imshow(a)\n",
    "        plt.show()\n",
    "print(\"Average loss: \", average_loss / len(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(TrainingPaths[0])):\n",
    "    #x = np.load(data[0][0][i])\n",
    "    #y = np.load(data[0][1][i])\n",
    "    x = np.load(TrainingPaths[0][i])\n",
    "    y = np.load(TrainingPaths[1][i])\n",
    "    a = np.reshape(vae(np.reshape(x, (1, 128, 128, 1))), (128, 128))\n",
    "    print(TrainingPaths[0][i])\n",
    "\n",
    "    w=10\n",
    "    h=10\n",
    "    fig=plt.figure(figsize=(8, 8))\n",
    "    columns = 3\n",
    "    rows = 1\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(x)\n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow(y)\n",
    "    fig.add_subplot(rows, columns, 3)\n",
    "    plt.imshow(a)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
